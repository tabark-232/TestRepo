#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
import re
import json
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import tokenizer_from_json

# ============= ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙ†ÙŠØ²ÙŠØ´Ù† ============
@st.cache_resource
def load_model_and_tokenizer():
    model = tf.keras.models.load_model("sentiment_model.h5")
    with open("tokenizer.json") as f:
        data = json.load(f)
        tokenizer = tokenizer_from_json(data)
    return model, tokenizer

model, tokenizer = load_model_and_tokenizer()

max_len = 100  # ØªØ£ÙƒØ¯ Ø£Ù†Ù‡Ø§ Ù†ÙØ³ Ø§Ù„Ù‚ÙŠÙ…Ø© ÙŠÙ„ÙŠ Ø§Ø³ØªØ®Ø¯Ù…ØªÙ‡Ø§ ÙˆÙ‚Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨

# ============= ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ ============
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    text = re.sub(r'\W', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# ============= Ø£Ø³Ø¨Ø§Ø¨ Ù…Ø­ØªÙ…Ù„Ø© ============
def extract_reason(comment):
    reasons = {
        "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª": ["Ø¥Ø¹Ù„Ø§Ù†Ø§Øª", "Ø§Ø¹Ù„Ø§Ù†Ø§Øª", "Ø¥Ø¹Ù„Ø§Ù†", "ads", "ad"],
        "Ø¨Ø·Ø¡": ["Ø¨Ø·ÙŠØ¡", "Ø¨Ø·ÙŠØ¦", "Ø¨Ø·Ø¦", "ÙŠØ¹Ù„Ù‚", "ØªØ£Ø®ÙŠØ±", "Ø¨Ø·ÙŠØ¦Ù‡"],
        "Ø£Ø®Ø·Ø§Ø¡": ["ÙƒØ±Ø§Ø´", "Ø®Ø·Ø£", "ØªØ¹Ù„ÙŠÙ‚", "Ù…Ø§ ÙŠØ´ØªØºÙ„", "Ù„Ø§ ÙŠØ¹Ù…Ù„", "ÙŠÙ‡Ù†Ù‚"],
        "Ø³Ø¹Ø±": ["ØºØ§Ù„ÙŠ", "Ø³Ø¹Ø±", "ÙÙ„ÙˆØ³", "Ø¯ÙØ¹", "Ù…Ø¨Ù„Øº"],
    }

    comment = comment.lower()
    for reason, keywords in reasons.items():
        for word in keywords:
            if word in comment:
                return reason
    return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"

# ============= ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ============
st.title("ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ğŸ§ ğŸ“Š")
st.write("Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ Ø§Ø³Ù…Ù‡ `comment`")

uploaded_file = st.file_uploader("ğŸ“¤ Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù", type="csv")

if uploaded_file:
    df = pd.read_csv(uploaded_file)

    if 'comment' not in df.columns:
        st.error("âŒ Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ Ø¨Ø§Ø³Ù… 'comment'")
    else:
        st.success("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­")
        df['clean'] = df['comment'].apply(clean_text)
        sequences = tokenizer.texts_to_sequences(df['clean'])
        padded = pad_sequences(sequences, maxlen=max_len)

        predictions = model.predict(padded)
        df['prediction'] = (predictions > 0.5).astype(int)
        df['Ø§Ù„Ù†ØªÙŠØ¬Ø©'] = df['prediction'].apply(lambda x: "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ" if x == 1 else "Ø³Ù„Ø¨ÙŠ")
        df['Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø­ØªÙ…Ù„'] = df.apply(lambda row: extract_reason(row['comment']) if row['Ø§Ù„Ù†ØªÙŠØ¬Ø©'] == "Ø³Ù„Ø¨ÙŠ" else "", axis=1)

        st.subheader("ğŸ“‹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        st.dataframe(df[['comment', 'Ø§Ù„Ù†ØªÙŠØ¬Ø©', 'Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø­ØªÙ…Ù„']])

        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ…Ù„Ù CSV", csv, "predicted_results.csv", "text/csv")

